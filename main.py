# -*- coding: utf-8 -*-
"""Predicting Hard Drive Failures with ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4WCHugB8n28tXuTKf2Tkk8ewMxOPsHz

# Прогнозирование сбоев жесткого диска с помощью ML
Использование машинного обучения для построения модели, которая может предсказать вероятность отказа жесткого диска.

# Predicting Hard Drive Failures with ML
Using Machine Learning to build a model that can predict the likelihood of a hard disk failure.

__________________________________________________________________________
__________________________________________________________________________

## 0. Подготовка среды выполнения
## 0. Preparing the environment

__________________________________________________________________________
"""

# Получение информации о процессоре
!lscpu

# Получение информации об оперативной памяти
!cat /proc/meminfo | grep 'MemAvailable'

# Получение информации о дисковом пространстве
!df -h

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats
from sklearn.utils import resample

import os
import csv
import glob
import time
import zipfile
from google.colab import drive

drive.mount('/content/drive') # connect to google drive

# Path to the archive file
# zip_file = '/content/drive/My Drive/Predicting_HardDrive_Failures_with_ML/dataset.zip'  # for prod
zip_file = '/content/drive/My Drive/Predicting_HardDrive_Failures_with_ML/test.zip'       # for test

# Unpacking the data into the 'dataset' folder
with zipfile.ZipFile(zip_file, 'r') as z:
    z.extractall('/content/dataset')

print(f'[ Каталоги ] >>> \n{os.listdir()}')

"""## 1. Исследовательский анализ данных
## 1. Exploratory Data Analysis (EDA)

__________________________________________________________________________

"""

def split_and_save_csv_files(path):
  import shutil
  from sklearn.model_selection import train_test_split

  filenames = glob.glob(os.path.join(path, "*.csv"))

  dfs = []

  for filename in filenames:
    dfs.append(pd.read_csv(filename))

  data = pd.concat(dfs, ignore_index=True)

  data_train, data_test = train_test_split(data, test_size=0.3, random_state=42)

  data_train.to_csv('/content/data_train.csv', index=False)
  data_test.to_csv('/content/data_test.csv', index=False)

  shutil.rmtree(path)


split_and_save_csv_files('/content/dataset')

# Сделать проврку, что если неисправнфх в перемешенном меньше чем 10%, то перемешать еще раз

df_train = pd.read_csv('data_train.csv').sample(frac=1).reset_index(drop=True).head(100000)
df_test = pd.read_csv('data_test.csv').sample(frac=1).reset_index(drop=True).head(30000)

df_train.head()

import seaborn as sns

df_train[['smart_5_normalized','smart_187_normalized','smart_188_normalized','smart_197_normalized','smart_198_normalized']].sample(frac=1).head(10000).plot(kind='barh', figsize=(10, 6))
plt.xlabel('Значение')
plt.ylabel('Модель')
plt.title('График столбцов SMART-атрибутов')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.show()

df_train[['capacity_bytes',	'failure',	'smart_5_normalized','smart_187_normalized','smart_188_normalized','smart_197_normalized','smart_198_normalized']].describe()

df_test.head()

df_test[['smart_5_normalized','smart_187_normalized','smart_188_normalized','smart_197_normalized','smart_198_normalized']].sample(frac=1).head(10000).plot(kind='barh', figsize=(10, 6))
plt.xlabel('Значение')
plt.ylabel('Модель')
plt.title('График столбцов SMART-атрибутов')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.show()

df_test[['capacity_bytes',	'failure',	'smart_5_normalized','smart_187_normalized','smart_188_normalized','smart_197_normalized','smart_198_normalized']].describe()

print(f'[ Размер тренеровочной выборки ] >>> {df_train.shape}')
print(f'[ Размер тестовой выборки ] >>> {df_test.shape}')

"""### Распределение целевой переменной (Сбои)
### Distribution of target variable (Failure)

__________________________________________________________________________
"""

target = 'failure' # target variable

# Checking the distribution of the target variable
sns.countplot(df_train['failure'])

"""### Балансировка набора данных
### Balancing the dataset

__________________________________________________________________________
"""

valid = df_train[df_train['failure'] == 0]    # data of HDDs which do not indicate failure
failed = df_train[df_train['failure'] == 1]   # data of HDDs likely to fail

print("[ Число рабочих HDDs ] >>> ", len(valid))        # storing the total number of valid HDDs
print("[ Число неисправных HDDs ] >>> ", len(failed))   # storing the total number of HDDs likely to fail

"""Поскольку количество жестких дисков, указывающих на сбой, слишком мало, приступим к повышению дискретизации класса меньшинства, а именно «сбой».
Выполняем этот шаг, чтобы предотвратить предвзятость окончательной модели.
Повторная выборка класса отказа для соответствия длине действительных жестких дисков.

Since the number of HDDs indicating failure are too low, we proceed to upsample the minority class viz.'failure'. We perform this step to prevent our final model from being biased. Resampling of the failure class to match the length of valid HDDs.

__________________________________________________________________________
"""

if len(valid) > 0:
    failed_up = resample(failed, replace=True, n_samples=len(valid), random_state=27)

else:
    print("Недостаточно данных в переменной 'valid'")

""" Наконец, объединяем наши недавно преобразованные классы с нашими обучающими данными.

 Finally we concatenate our newly resampled classes with our training data.

 _________________________________________________________________________
"""

df_train = pd.concat([valid,failed_up])
df_train.failure.value_counts()         # Levelling the count of both classes

sns.countplot(df_train['failure'])

df_train

"""Можно заметить, что размеры набора обучающих данных увеличились вдвое.

You can notice the dimensions have doubled for our training dataset.

__________________________________________________________________________
"""

df_train.shape

"""### Функция Выбор и заполнение недостающих значений
### Feature Selection and filling of missing values

_________________________________________________________________________
"""

# For the training data
df_train.isnull().sum()

"""Выбор функций с высокой корреляцией с отказом жесткого диска:
функции, которые сильно коррелируют с отказом жесткого диска согласно
[BackBlaze](https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/ "Переход на официальный сайт")

High-precision, hard drive-free selection features:
features that are highly correlated with hard drive failure according to
[BackBlaze](https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/ "Go to the official website")

__________________________________________________________________________
"""

# SMART 5 		  Reallocated Sectors Count
# SMART 187 		Reported Uncorrectable Errors
# SMART 188 		Command Timeout
# SMART 197 		Current Pending Sector Count
# SMART 198 		Uncorrectable Sector Count

features = [
    'date',
    'serial_number',
    'model',
    'capacity_bytes',
    'failure',
    'smart_5_raw','smart_187_raw','smart_188_raw','smart_197_raw','smart_198_raw'
]

misc_feat = [fname for fname in df_train if fname not in features]  # misc features to be dropped
misc_feat

df_train.drop(misc_feat, inplace=True, axis=1)  # Dropping the misc features

df_train

# Since our model cannot proccess string values, we remove the columns which contain string values/object values
# to avoid errors

obj = df_train.dtypes[df_train.dtypes == object ].index
obj

df_train = df_train.drop(obj,axis=1)

# Handling missing values
df_train.isnull().sum()  #Total number of missing values

#After going through the dataset,i found that the drives which were missing values did not correlate to its failure

#  i.e all drives indicating failure did not contain missing values

# Hence i replaced them with the most commonly occuring values for the respective SMART attributes

df_train['smart_187_raw'] = df_train['smart_187_raw'].fillna(0)

df_train['smart_5_raw'] = df_train['smart_5_raw'].fillna(0)

df_train['smart_188_raw'] = df_train['smart_188_raw'].fillna(0)

df_train['smart_197_raw'] = df_train['smart_197_raw'].fillna(0)

df_train['smart_198_raw'] = df_train['smart_198_raw'].fillna(0)

df_train.isnull().sum()

df_train = df_train.drop('capacity_bytes',axis=1)

df_train

# Splitting the values for X_train and Y_train

X_train = df_train.drop('failure',axis=1)
Y_train = df_train['failure']

# For Test data:

# Upsampling of test data to match the dimensionality of the test and train data (optional) #### Note : You can skip this step if using TrainTestSplit function

valid_test = df_test[df_test['failure'] == 0]
failed_test = df_test[df_test['failure'] == 1]

print("valid hdds:",len(valid_test))
print("failing hdds:",len(failed_test))

failed_up_test = resample(failed,replace=True,n_samples=len(valid),random_state=27) # Same steps as in Training data

df_test = pd.concat([valid_test,failed_up_test])
df_test.failure.value_counts()

df_test.head()

df_test.shape

# Feature Selection for test data

df_test.drop(misc_feat,inplace=True,axis=1) # Since we have the imp features, we move ahead to drop the misc ones

df_test

# Filling out missing values for test data

# We perform this step as our model cannot use NaN data

df_test['smart_187_raw'] = df_test['smart_187_raw'].fillna(0)

df_test['smart_5_raw'] = df_test['smart_5_raw'].fillna(0)

df_test['smart_188_raw'] = df_test['smart_188_raw'].fillna(0)

df_test['smart_197_raw'] =df_test['smart_197_raw'].fillna(0)

df_test['smart_198_raw'] = df_test['smart_198_raw'].fillna(0)

df_test.isnull().sum()

df_test = df_test.drop(obj,axis=1)

df_test = df_test.drop('capacity_bytes',axis=1)

df_test

"""### Splitting values for X_test and Y_test (Optional)

Note: Please skip this step if using TrainTestSpilt Method

"""

X_test = df_test.drop('failure',axis=1)

Y_test = df_test['failure']

df_test.shape

"""## 2. Building the model using Random Forest:

### Model 1: RF Using X_test and Y_test
Note : Please refer Model 2 if using Train_Test_Split
"""

#Building the Random Forest Classifier (RANDOM FOREST)
from sklearn.ensemble import RandomForestClassifier

# RF model creation
rfc = RandomForestClassifier()
rfc.fit(X_train, Y_train)

# predictions(Notice the caps'P' of yPred to differentiate between model 1 and 2)
yPred = rfc.predict(X_test)

# Results of our predictions

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, matthews_corrcoef
from sklearn.metrics import confusion_matrix

n_outliers = len(failed)
n_errors = (yPred != Y_test).sum()        # Notice the Y_test from iii) of Test Data
print("Model used is: Random Forest classifier")

acc = accuracy_score(Y_test, yPred)
print("The accuracy is {}".format(acc))

prec = precision_score(Y_test, yPred)
print("The precision is {}".format(prec))

rec = recall_score(Y_test, yPred)
print("The recall is {}".format(rec))

f1 = f1_score(Y_test, yPred)
print("The F1-Score is {}".format(f1))

MCC = matthews_corrcoef(Y_test, yPred)
print("The Matthews correlation coefficient is {}".format(MCC))

"""### Визуализация

### Visualization

__________________________________________________________________________
"""

# confusion matrix

LABELS = ['Healthy', 'Failed']
conf_matrix = confusion_matrix(Y_test, yPred)
plt.figure(figsize =(12, 12))
sns.heatmap(conf_matrix, xticklabels = LABELS,
            yticklabels = LABELS, annot = True, fmt ="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()

"""### Model 2: RF Using the Train_Test_Split Method"""

xData = X_train.values
yData = Y_train.values

from sklearn.model_selection import train_test_split

# Splitting of data into training and testing sets
xTrain, xTest, yTrain, yTest = train_test_split(
        xData, yData, test_size = 0.2, random_state = 42)

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

# RF model creation
rfc = RandomForestClassifier()
rfc.fit(xTrain, yTrain)

# predictions (notice the small 'p' to differentiate from model 1)
ypred = rfc.predict(xTest)

from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, matthews_corrcoef
from sklearn.metrics import confusion_matrix

n_outliers = len(failed)
n_errors = (ypred != yTest).sum()                             # yTest from the Train_Test_Split function
print("Model used is : Random Forest classifier")

acc = accuracy_score(yTest, ypred)
print("The accuracy is {}".format(acc))

prec = precision_score(yTest, ypred)
print("The precision is {}".format(prec))

rec = recall_score(yTest, ypred)
print("The recall is {}".format(rec))

f1 = f1_score(yTest, ypred)
print("The F1-Score is {}".format(f1))

MCC = matthews_corrcoef(yTest, ypred)
print("The Matthews correlation coefficient is{}".format(MCC))

"""### Визуализация

### Visualization

__________________________________________________________________________
"""

# confusion matrix

LABELS = ['Normal', 'Failed']
conf_matrix = confusion_matrix(yTest, ypred)
plt.figure(figsize =(12, 12))
sns.heatmap(conf_matrix, xticklabels = LABELS,
            yticklabels = LABELS, annot = True, fmt ="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Получите первое дерево из леса
single_tree = rfc.estimators_[0]

# Постройте дерево
plt.figure(figsize=(20, 10))
plot_tree(single_tree, filled=True, feature_names=X_train.columns, class_names=True)
plt.show()

"""## TEST
### RF regrassion y(x) = cos(x) - sin(x/2) + n(x)
"""

from sklearn.ensemble import RandomForestRegressor
import numpy as np
import matplotlib.pyplot as plt


x = np.arange(0, np.pi, 0.1)
n_samples = len(x)
y = np.cos(x) - np.sin(x/2) + np.random.normal(0.0, 0.1, n_samples)
x = x.reshape(-1, 1)

clf = RandomForestRegressor(max_depth=2, n_estimators=1, random_state=1)
clf.fit(x, y)
yy = clf.predict(x)

plt.plot(x, y, label="cos(x) - sin(x/2)")
plt.plot(x, yy, label="DT Regression")
plt.grid()
plt.legend()
plt.title('Одно дерево глубиной 2')
plt.show()